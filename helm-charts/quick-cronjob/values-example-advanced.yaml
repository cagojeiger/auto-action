# Quick-CronJob Advanced Examples
# Production-ready configurations with complex requirements

global:
  imageRegistry: ""
  imagePullSecrets: []
  timezone: "Asia/Seoul"

jobs:
  # Production PostgreSQL backup with S3 upload
  postgres-backup:
    image:
      repository: postgres
      tag: 15-alpine
    schedule: "0 2 * * *"
    timeZone: "Asia/Seoul"
    suspend: true  # Start suspended for testing

    # Production-grade backup script (modifiable in field)
    configMap:
      create: true
      data:
        backup.sh: |
          #!/bin/bash
          set -euo pipefail

          # Configuration
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="/backup/db_${TIMESTAMP}.sql"
          S3_BUCKET="${S3_BUCKET:-my-backups}"
          RETENTION_DAYS="${RETENTION_DAYS:-30}"

          echo "Starting PostgreSQL backup: $TIMESTAMP"

          # Create backup
          pg_dump $DATABASE_URL \
            --format=custom \
            --no-acl \
            --no-owner \
            --file="$BACKUP_FILE"

          echo "Backup created: $BACKUP_FILE"

          # Compress
          gzip "$BACKUP_FILE"
          BACKUP_FILE="${BACKUP_FILE}.gz"

          # Upload to S3 (if configured)
          if [ -n "${AWS_ACCESS_KEY_ID:-}" ]; then
            echo "Uploading to S3: s3://$S3_BUCKET/$(basename $BACKUP_FILE)"
            aws s3 cp "$BACKUP_FILE" "s3://$S3_BUCKET/"

            # Cleanup old backups in S3
            aws s3 ls "s3://$S3_BUCKET/" | \
              awk '{print $4}' | \
              grep 'db_.*\.sql\.gz$' | \
              sort -r | \
              tail -n +$((RETENTION_DAYS + 1)) | \
              xargs -I {} aws s3 rm "s3://$S3_BUCKET/{}"
          fi

          # Local cleanup
          find /backup -name "db_*.sql.gz" -mtime +$RETENTION_DAYS -delete

          echo "Backup completed successfully"

        restore.sh: |
          #!/bin/bash
          set -euo pipefail

          if [ $# -eq 0 ]; then
            echo "Usage: $0 <backup_file>"
            echo "Available backups:"
            ls -la /backup/db_*.sql.gz 2>/dev/null || echo "No backups found"
            exit 1
          fi

          BACKUP_FILE="$1"
          echo "Restoring from: $BACKUP_FILE"

          # Restore database
          zcat "$BACKUP_FILE" | pg_restore \
            --dbname="$DATABASE_URL" \
            --clean \
            --if-exists \
            --no-acl \
            --no-owner

          echo "Restore completed"

    env:
      - name: DATABASE_URL
        valueFrom:
          secretKeyRef:
            name: postgres-credentials
            key: url
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: aws-credentials
            key: access-key-id
            optional: true
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: aws-credentials
            key: secret-access-key
            optional: true
      - name: S3_BUCKET
        value: "my-production-backups"
      - name: RETENTION_DAYS
        value: "30"

    command: ["/bin/bash", "/scripts/backup.sh"]

    persistence:
      enabled: true
      size: 50Gi
      storageClass: "fast-ssd"
      mountPath: /backup

    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "1"

    testDeployment:
      enabled: true
      command: ["sleep", "infinity"]

  # ETL pipeline with complex dependencies
  etl-processor:
    image:
      repository: python
      tag: 3.11-slim
    schedule: "0 4 * * *"
    suspend: true

    # ETL script with error handling and monitoring
    configMap:
      create: true
      data:
        etl.py: |
          #!/usr/bin/env python3
          import os
          import sys
          import json
          import time
          import logging
          from datetime import datetime

          # Setup logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger(__name__)

          def send_metrics(metric_name, value, tags=None):
              """Send metrics to monitoring system"""
              # Implement your metrics sending logic
              logger.info(f"METRIC: {metric_name}={value} {tags or ''}")

          def process_data():
              """Main ETL processing logic"""
              start_time = time.time()

              try:
                  logger.info("Starting ETL process")

                  # Extract
                  logger.info("Extracting data...")
                  # Add your extraction logic here

                  # Transform
                  logger.info("Transforming data...")
                  # Add your transformation logic here

                  # Load
                  logger.info("Loading data...")
                  # Add your loading logic here

                  duration = time.time() - start_time
                  send_metrics("etl.duration", duration, "status=success")
                  logger.info(f"ETL completed successfully in {duration:.2f}s")

              except Exception as e:
                  duration = time.time() - start_time
                  send_metrics("etl.duration", duration, "status=error")
                  logger.error(f"ETL failed after {duration:.2f}s: {e}")
                  sys.exit(1)

          if __name__ == "__main__":
              process_data()

        requirements.txt: |
          requests==2.31.0
          pandas==2.0.3
          sqlalchemy==2.0.21
          psycopg2-binary==2.9.7

        install-deps.sh: |
          #!/bin/bash
          echo "Installing Python dependencies..."
          pip install --no-cache-dir -r /scripts/requirements.txt
          echo "Dependencies installed"

    command: ["/bin/bash"]
    args:
      - -c
      - |
        /scripts/install-deps.sh
        python3 /scripts/etl.py

    env:
      - name: DATABASE_URL
        valueFrom:
          secretKeyRef:
            name: database-credentials
            key: url
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            name: api-credentials
            key: key
      - name: ENVIRONMENT
        value: "production"

    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"

    testDeployment:
      enabled: true
      command: ["sleep", "infinity"]
      # Override for testing
      env:
        - name: ENVIRONMENT
          value: "testing"

  # Log aggregation and shipping
  log-shipper:
    image:
      repository: fluent/fluent-bit
      tag: 2.2.0
    schedule: "*/30 * * * *"  # Every 30 minutes
    suspend: true

    serviceAccount:
      create: true

    clusterRole:
      create: true
      rules:
        - apiGroups: [""]
          resources: ["pods", "namespaces"]
          verbs: ["get", "list"]
        - apiGroups: [""]
          resources: ["pods/log"]
          verbs: ["get"]

    configMap:
      create: true
      data:
        fluent-bit.conf: |
          [SERVICE]
              Flush         5
              Log_Level     info
              Daemon        off
              Parsers_File  parsers.conf

          [INPUT]
              Name              tail
              Path              /var/log/containers/*.log
              Parser            docker
              Tag               kube.*
              Refresh_Interval  5
              Mem_Buf_Limit     50MB
              Skip_Long_Lines   On

          [FILTER]
              Name                kubernetes
              Match               kube.*
              Kube_URL            https://kubernetes.default.svc:443
              Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token

          [OUTPUT]
              Name  forward
              Match *
              Host  ${FLUENTD_HOST}
              Port  ${FLUENTD_PORT}

        parsers.conf: |
          [PARSER]
              Name   docker
              Format json
              Time_Key time
              Time_Format %Y-%m-%dT%H:%M:%S.%L
              Time_Keep On

    env:
      - name: FLUENTD_HOST
        value: "fluentd.logging.svc.cluster.local"
      - name: FLUENTD_PORT
        value: "24224"

    command: ["/fluent-bit/bin/fluent-bit"]
    args: ["--config=/scripts/fluent-bit.conf"]

    volumeMounts:
      - name: varlog
        mountPath: /var/log
        readOnly: true
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true

    volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

    nodeSelector:
      kubernetes.io/os: linux

    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule

    testDeployment:
      enabled: false  # Skip test deployment for this complex job